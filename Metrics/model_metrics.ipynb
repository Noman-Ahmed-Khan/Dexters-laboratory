{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the detailed explanation of each metric along with its corresponding mathematical formula. All of the original information is preserved, with the formulas added for deeper study.\n",
    "# 1. Classification Metrics\n",
    "\n",
    "## Binary & Multiclass Classification\n",
    "\n",
    "### **Accuracy**  \n",
    "**What it tells:**  \n",
    "The overall proportion of correct predictions. It measures the rate at which the model predicts the correct class.\n",
    "\n",
    "**Consideration:**  \n",
    "May be misleading for imbalanced datasets where one class dominates.\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$  \n",
    "where:  \n",
    "- \\( TP \\) = True Positives  \n",
    "- \\( TN \\) = True Negatives  \n",
    "- \\( FP \\) = False Positives  \n",
    "- \\( FN \\) = False Negatives\n",
    "\n",
    "---\n",
    "\n",
    "### **Precision**  \n",
    "**What it tells:**  \n",
    "The proportion of positive predictions that are actually correct. It answers: \"When the model predicts positive, how often is it right?\"\n",
    "\n",
    "**Consideration:**  \n",
    "High precision means few false positives.\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Recall (Sensitivity, True Positive Rate - TPR)**  \n",
    "**What it tells:**  \n",
    "The proportion of actual positives that the model correctly identified. It answers: \"How many of the actual positive cases did the model capture?\"\n",
    "\n",
    "**Consideration:**  \n",
    "High recall means few false negatives.\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **F1 Score**  \n",
    "**What it tells:**  \n",
    "The harmonic mean of precision and recall. It provides a balance between the two, especially useful when you need a single metric for imbalanced datasets.\n",
    "\n",
    "**Consideration:**  \n",
    "A high F1 score indicates both high precision and recall.\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **ROC-AUC (Receiver Operating Characteristic - Area Under Curve)**  \n",
    "**What it tells:**  \n",
    "Measures the model’s ability to distinguish between classes by comparing the true positive rate to the false positive rate at various threshold settings.\n",
    "\n",
    "**Consideration:**  \n",
    "A higher AUC indicates better performance across different classification thresholds.\n",
    "\n",
    "**Formula:**  \n",
    "There is no single closed-form expression. It is computed as the area under the ROC curve, where:\n",
    "$$\n",
    "\\text{TPR} = \\frac{TP}{TP + FN} \\quad \\text{and} \\quad \\text{FPR} = \\frac{FP}{FP + TN}\n",
    "$$  \n",
    "Numerical integration (e.g., using the trapezoidal rule) is typically used.\n",
    "\n",
    "---\n",
    "\n",
    "### **PR-AUC (Precision-Recall AUC)**  \n",
    "**What it tells:**  \n",
    "Summarizes the trade-off between precision and recall across different thresholds, especially informative when dealing with imbalanced classes.\n",
    "\n",
    "**Consideration:**  \n",
    "More sensitive to the performance on the minority class than ROC-AUC.\n",
    "\n",
    "**Formula:**  \n",
    "Like ROC-AUC, PR-AUC is the area under the precision-recall curve:\n",
    "$$\n",
    "\\text{PR-AUC} = \\int_{0}^{1} \\text{Precision}(\\text{Recall}^{-1}(x)) \\, dx\n",
    "$$  \n",
    "computed numerically.\n",
    "\n",
    "---\n",
    "\n",
    "### **Log Loss (Cross-Entropy Loss)**  \n",
    "**What it tells:**  \n",
    "Measures the uncertainty of predictions by penalizing confident but wrong predictions more than less confident ones. Lower log loss indicates better model calibration.\n",
    "\n",
    "**Consideration:**  \n",
    "It’s sensitive to how well the predicted probabilities reflect true likelihoods.\n",
    "\n",
    "**Formula (for binary classification):**\n",
    "$$\n",
    "\\text{Log Loss} = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(\\hat{p}_i) + (1-y_i) \\log(1-\\hat{p}_i) \\right]\n",
    "$$  \n",
    "where:  \n",
    "- \\( y_i \\) is the true label (0 or 1)  \n",
    "- \\( \\hat{p}_i \\) is the predicted probability for the positive class  \n",
    "- \\( N \\) is the number of samples\n",
    "\n",
    "---\n",
    "\n",
    "## Multilabel Classification\n",
    "\n",
    "### **Hamming Loss**  \n",
    "**What it tells:**  \n",
    "The fraction of labels that are incorrectly predicted. It penalizes each misclassified label equally.\n",
    "\n",
    "**Consideration:**  \n",
    "Lower values indicate better performance.\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "\\text{Hamming Loss} = \\frac{1}{N \\times L} \\sum_{i=1}^{N} \\sum_{j=1}^{L} \\mathbf{1}(y_{ij} \\neq \\hat{y}_{ij})\n",
    "$$  \n",
    "where:  \n",
    "- \\( N \\) is the number of samples  \n",
    "- \\( L \\) is the number of labels per sample  \n",
    "- \\( \\mathbf{1}(\\cdot) \\) is the indicator function\n",
    "\n",
    "---\n",
    "\n",
    "### **Jaccard Similarity (Intersection over Union)**  \n",
    "**What it tells:**  \n",
    "Measures the similarity between the predicted set of labels and the true set of labels. It is the size of the intersection divided by the size of the union of the label sets.\n",
    "\n",
    "**Consideration:**  \n",
    "Higher values indicate better overlap between predictions and true labels.\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "\\text{Jaccard Similarity} = \\frac{|Y \\cap \\hat{Y}|}{|Y \\cup \\hat{Y}|}\n",
    "$$  \n",
    "where:  \n",
    "- \\( Y \\) is the set of true labels  \n",
    "- \\( \\hat{Y} \\) is the set of predicted labels\n",
    "\n",
    "---\n",
    "\n",
    "## Imbalanced Classification\n",
    "\n",
    "### **Balanced Accuracy**  \n",
    "**What it tells:**  \n",
    "The average recall obtained on each class, which helps when classes are imbalanced.\n",
    "\n",
    "**Consideration:**  \n",
    "It adjusts for the class imbalance by taking the average of the per-class recall values.\n",
    "\n",
    "**Formula (for \\( C \\) classes):**\n",
    "$$\n",
    "\\text{Balanced Accuracy} = \\frac{1}{C} \\sum_{c=1}^{C} \\frac{TP_c}{TP_c + FN_c}\n",
    "$$  \n",
    "where \\( TP_c \\) and \\( FN_c \\) are the true positives and false negatives for class \\( c \\).\n",
    "\n",
    "---\n",
    "\n",
    "### **Matthews Correlation Coefficient (MCC)**  \n",
    "**What it tells:**  \n",
    "A correlation coefficient between the observed and predicted classifications. It takes into account true and false positives and negatives and is regarded as a balanced measure even if the classes are of very different sizes.\n",
    "\n",
    "**Consideration:**  \n",
    "Ranges from -1 (total disagreement) to +1 (perfect prediction).\n",
    "\n",
    "**Formula (binary case):**\n",
    "$$\n",
    "\\text{MCC} = \\frac{TP \\times TN - FP \\times FN}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **F1 Score (macro, weighted, or micro)**  \n",
    "**What it tells:**  \n",
    "Variants of the F1 score adjust for class imbalance:  \n",
    "- **Macro F1:** Averages F1 scores per class without weighting, treating all classes equally.  \n",
    "- **Weighted F1:** Averages F1 scores per class, weighted by the number of instances in each class.  \n",
    "- **Micro F1:** Aggregates contributions of all classes to compute the average metric.\n",
    "\n",
    "**Consideration:**  \n",
    "The choice depends on whether you want to give equal importance to all classes or weigh them by frequency.\n",
    "\n",
    "**Formula:**  \n",
    "The base F1 formula is:\n",
    "$$\n",
    "F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$  \n",
    "The aggregation method (macro, weighted, micro) determines how the scores are averaged over the classes.\n",
    "\n",
    "---\n",
    "\n",
    "# 2. Regression Metrics\n",
    "\n",
    "### **Mean Absolute Error (MAE)**  \n",
    "**What it tells:**  \n",
    "The average absolute difference between the predicted and actual values. It gives a straightforward measure of prediction error in the same units as the output.\n",
    "\n",
    "**Consideration:**  \n",
    "All errors are weighted equally, regardless of their magnitude.\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "MAE = \\frac{1}{N} \\sum_{i=1}^{N} \\left| y_i - \\hat{y}_i \\right|\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Mean Squared Error (MSE)**  \n",
    "**What it tells:**  \n",
    "The average of the squared differences between predicted and actual values. Squaring errors penalizes larger errors more significantly.\n",
    "\n",
    "**Consideration:**  \n",
    "Sensitive to outliers due to the squaring of differences.\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "MSE = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Root Mean Squared Error (RMSE)**  \n",
    "**What it tells:**  \n",
    "The square root of MSE, which converts the error metric back to the original units of the output.\n",
    "\n",
    "**Consideration:**  \n",
    "Like MSE, it penalizes large errors, making it useful when large errors are particularly undesirable.\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "RMSE = \\sqrt{MSE} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **R-squared (\\(R^2\\))**  \n",
    "**What it tells:**  \n",
    "The proportion of the variance in the dependent variable that is predictable from the independent variables. It ranges from 0 to 1 (sometimes negative if the model performs poorly).\n",
    "\n",
    "**Consideration:**  \n",
    "Higher values indicate a better fit of the model to the data.\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{N} (y_i - \\bar{y})^2}\n",
    "$$  \n",
    "where \\( \\bar{y} \\) is the mean of the actual values.\n",
    "\n",
    "---\n",
    "\n",
    "### **Mean Absolute Percentage Error (MAPE)**  \n",
    "**What it tells:**  \n",
    "The average absolute percentage difference between predicted and actual values, providing a relative measure of error.\n",
    "\n",
    "**Consideration:**  \n",
    "Can be problematic when actual values are close to zero.\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "MAPE = \\frac{100\\%}{N} \\sum_{i=1}^{N} \\left| \\frac{y_i - \\hat{y}_i}{y_i} \\right|\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Huber Loss**  \n",
    "**What it tells:**  \n",
    "A combination of MAE and MSE that is less sensitive to outliers than MSE while still differentiable at zero error.\n",
    "\n",
    "**Consideration:**  \n",
    "Useful in scenarios where outliers may otherwise skew the error metric.\n",
    "\n",
    "**Formula:**  \n",
    "For a given threshold \\( \\delta \\) and error \\( a = y_i - \\hat{y}_i \\):\n",
    "$$\n",
    "L_\\delta(a) =\n",
    "\\begin{cases}\n",
    "\\frac{1}{2}a^2, & \\text{if } |a| \\le \\delta \\\\\n",
    "\\delta \\left(|a| - \\frac{1}{2}\\delta\\right), & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "# 3. Clustering Metrics\n",
    "\n",
    "### **Silhouette Score**  \n",
    "**What it tells:**  \n",
    "Measures how similar an object is to its own cluster compared to other clusters. Scores range from -1 (poor clustering) to +1 (well-clustered), with values around 0 indicating overlapping clusters.\n",
    "\n",
    "**Consideration:**  \n",
    "Useful when the true number of clusters is unknown.\n",
    "\n",
    "**Formula (for a single sample \\( i \\)):**\n",
    "$$\n",
    "s(i) = \\frac{b(i) - a(i)}{\\max\\{a(i),\\, b(i)\\}}\n",
    "$$  \n",
    "where:  \n",
    "- \\( a(i) \\) is the average intra-cluster distance for sample \\( i \\).  \n",
    "- \\( b(i) \\) is the average distance from sample \\( i \\) to the nearest cluster it does not belong to.\n",
    "\n",
    "---\n",
    "\n",
    "### **Davies-Bouldin Index**  \n",
    "**What it tells:**  \n",
    "Evaluates intra-cluster similarity (how close data points are within the same cluster) and inter-cluster separation (how distinct the clusters are from each other). Lower values indicate better clustering.\n",
    "\n",
    "**Consideration:**  \n",
    "It is sensitive to the number of clusters chosen.\n",
    "\n",
    "**Formula (for \\( K \\) clusters):**\n",
    "$$\n",
    "DB = \\frac{1}{K} \\sum_{i=1}^{K} \\max_{j \\neq i} \\left( \\frac{S_i + S_j}{M_{ij}} \\right)\n",
    "$$  \n",
    "where:  \n",
    "- \\( S_i \\) is the average distance of all points in cluster \\( i \\) to its centroid, and  \n",
    "- \\( M_{ij} \\) is the distance between the centroids of clusters \\( i \\) and \\( j \\).\n",
    "\n",
    "---\n",
    "\n",
    "### **Dunn Index**  \n",
    "**What it tells:**  \n",
    "The ratio between the smallest distance between observations not in the same cluster (inter-cluster distance) and the largest intra-cluster distance. Higher values indicate better clustering quality.\n",
    "\n",
    "**Consideration:**  \n",
    "It is used to identify compact and well-separated clusters.\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "\\text{Dunn Index} = \\frac{\\min_{1 \\leq i < j \\leq K} \\delta(C_i, C_j)}{\\max_{1 \\leq k \\leq K} \\Delta(C_k)}\n",
    "$$  \n",
    "where:  \n",
    "- \\( \\delta(C_i, C_j) \\) is the inter-cluster distance between clusters \\( C_i \\) and \\( C_j \\), and  \n",
    "- \\( \\Delta(C_k) \\) is the intra-cluster distance (diameter) of cluster \\( C_k \\).\n",
    "\n",
    "---\n",
    "\n",
    "### **Adjusted Rand Index (ARI)**  \n",
    "**What it tells:**  \n",
    "Measures the similarity between the clustering result and the ground truth (if available), adjusted for chance. It ranges from -1 to 1, where 1 indicates perfect agreement.\n",
    "\n",
    "**Consideration:**  \n",
    "Especially useful when comparing different clustering algorithms or parameter settings.\n",
    "\n",
    "**Formula (simplified representation):**\n",
    "$$\n",
    "ARI = \\frac{RI - \\text{Expected } RI}{\\text{Max } RI - \\text{Expected } RI}\n",
    "$$  \n",
    "where \\( RI \\) (Rand Index) is computed based on pair-counting between clusters and ground truth labels.\n",
    "\n",
    "---\n",
    "\n",
    "### **Normalized Mutual Information (NMI)**  \n",
    "**What it tells:**  \n",
    "Measures the amount of shared information between the predicted clusters and the true clusters, normalized to scale between 0 and 1.\n",
    "\n",
    "**Consideration:**  \n",
    "A higher NMI means a better agreement between the cluster assignments and the actual labels.\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "NMI = \\frac{I(U;V)}{\\sqrt{H(U) \\, H(V)}}\n",
    "$$  \n",
    "where:  \n",
    "- \\( I(U;V) \\) is the mutual information between the clustering \\( U \\) and the ground truth \\( V \\), and  \n",
    "- \\( H(U) \\) and \\( H(V) \\) are the entropies of \\( U \\) and \\( V \\), respectively.\n",
    "\n",
    "---\n",
    "\n",
    "# 4. Anomaly Detection Metrics\n",
    "\n",
    "### **Precision, Recall, and F1-score**  \n",
    "**What they tell:**  \n",
    "Similar to classification, these metrics focus on the correct detection of anomalies (the minority class).  \n",
    "- **Precision:** How many detected anomalies are true anomalies.  \n",
    "- **Recall:** How many true anomalies were detected.  \n",
    "- **F1-score:** Balances precision and recall.\n",
    "\n",
    "**Consideration:**  \n",
    "Crucial for applications where false negatives (missed anomalies) or false positives (false alarms) have significant consequences.\n",
    "\n",
    "**Formulas:**  \n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "$$\n",
    "F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **ROC-AUC / PR-AUC**  \n",
    "**What they tell:**  \n",
    "Evaluate the performance of anomaly detection models by examining the trade-off between true positives and false positives (ROC-AUC) or precision and recall (PR-AUC), which is particularly important for imbalanced datasets.\n",
    "\n",
    "**Consideration:**  \n",
    "PR-AUC is often more informative in cases with a heavy imbalance.\n",
    "\n",
    "**Formula:**  \n",
    "Computed as the area under the respective curves (see Classification Metrics for details), typically using numerical integration.\n",
    "\n",
    "---\n",
    "\n",
    "### **Mean Squared Error (for reconstruction-based methods like Autoencoders)**  \n",
    "**What it tells:**  \n",
    "In models that reconstruct input data (e.g., autoencoders), the reconstruction error (often measured by MSE) can be used to identify anomalies. A higher error may indicate an anomaly.\n",
    "\n",
    "**Consideration:**  \n",
    "The threshold for anomaly detection must be chosen carefully.\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "MSE = \\frac{1}{N} \\sum_{i=1}^{N} (x_i - \\hat{x}_i)^2\n",
    "$$  \n",
    "where:  \n",
    "- \\( x_i \\) is the original input and \\( \\hat{x}_i \\) is the reconstructed input.\n",
    "\n",
    "---\n",
    "\n",
    "### **Z-score / Mahalanobis Distance**  \n",
    "**What they tell:**  \n",
    "These metrics measure how far a data point is from the mean (or the expected distribution) of the normal data.\n",
    "\n",
    "**Consideration:**  \n",
    "Useful in statistical anomaly detection where anomalies are assumed to be far from the mean or outside a certain distribution.\n",
    "\n",
    "**Z-score Formula:**\n",
    "$$\n",
    "z = \\frac{x - \\mu}{\\sigma}\n",
    "$$  \n",
    "where:  \n",
    "- \\( x \\) is the value,  \n",
    "- \\( \\mu \\) is the mean, and  \n",
    "- \\( \\sigma \\) is the standard deviation.\n",
    "\n",
    "**Mahalanobis Distance Formula:**\n",
    "$$\n",
    "D_M(x) = \\sqrt{(x - \\mu)^T \\Sigma^{-1} (x - \\mu)}\n",
    "$$  \n",
    "where:  \n",
    "- \\( x \\) is the data point,  \n",
    "- \\( \\mu \\) is the mean vector, and  \n",
    "- \\( \\Sigma \\) is the covariance matrix.\n",
    "\n",
    "---\n",
    "\n",
    "# 5. Ranking & Recommendation Metrics\n",
    "\n",
    "### **Mean Reciprocal Rank (MRR)**  \n",
    "**What it tells:**  \n",
    "The average of the reciprocal ranks of the first relevant item. It reflects how far down the list you have to go to find a relevant item on average.\n",
    "\n",
    "**Consideration:**  \n",
    "Useful when a single relevant result is enough, such as in search queries.\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "MRR = \\frac{1}{|Q|} \\sum_{i=1}^{|Q|} \\frac{1}{\\text{rank}_i}\n",
    "$$  \n",
    "where:  \n",
    "- \\( |Q| \\) is the number of queries, and  \n",
    "- \\( \\text{rank}_i \\) is the rank position of the first relevant item for the \\( i \\)th query.\n",
    "\n",
    "---\n",
    "\n",
    "### **Normalized Discounted Cumulative Gain (NDCG)**  \n",
    "**What it tells:**  \n",
    "Evaluates the quality of the ranking by giving higher scores to relevant items appearing higher in the list, with a discount for items lower down.\n",
    "\n",
    "**Consideration:**  \n",
    "Effective when relevance is graded (not just binary).\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "NDCG@k = \\frac{DCG@k}{IDCG@k}\n",
    "$$  \n",
    "where:\n",
    "$$\n",
    "DCG@k = \\sum_{i=1}^{k} \\frac{2^{\\text{rel}_i} - 1}{\\log_2(i+1)}\n",
    "$$  \n",
    "and \\( IDCG@k \\) is the ideal (maximum possible) DCG up to rank \\( k \\).\n",
    "\n",
    "---\n",
    "\n",
    "### **Hit Rate**  \n",
    "**What it tells:**  \n",
    "Measures whether at least one relevant item appears in the recommendation list for a user. It is usually expressed as a percentage.\n",
    "\n",
    "**Consideration:**  \n",
    "Does not consider the rank order of the hits.\n",
    "\n",
    "**Formula:**  \n",
    "There isn’t a single closed-form formula; it is often defined as:\n",
    "$$\n",
    "\\text{Hit Rate} = \\frac{\\text{Number of users with at least one hit}}{\\text{Total number of users}} \\times 100\\%\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Mean Average Precision (MAP)**  \n",
    "**What it tells:**  \n",
    "Averages the precision scores after each relevant item is retrieved, providing a single number summary of ranking quality across multiple queries.\n",
    "\n",
    "**Consideration:**  \n",
    "Sensitive to the ranking order and number of relevant items per query.\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "MAP = \\frac{1}{|Q|} \\sum_{q \\in Q} AP(q)\n",
    "$$  \n",
    "with\n",
    "$$\n",
    "AP(q) = \\frac{1}{N_q} \\sum_{k=1}^{n} P(k) \\times rel(k)\n",
    "$$  \n",
    "where:  \n",
    "- \\( N_q \\) is the number of relevant items for query \\( q \\),  \n",
    "- \\( P(k) \\) is the precision at rank \\( k \\), and  \n",
    "- \\( rel(k) \\) is an indicator function (1 if the item at rank \\( k \\) is relevant, 0 otherwise).\n",
    "\n",
    "---\n",
    "\n",
    "# 6. Generative Model Metrics\n",
    "\n",
    "### **Frechet Inception Distance (FID)**  \n",
    "**What it tells:**  \n",
    "Compares the distribution of generated images to real images by measuring the distance between feature representations (usually using a pretrained network). Lower FID indicates generated images that are more similar to real ones.\n",
    "\n",
    "**Consideration:**  \n",
    "Widely used in evaluating GANs and other generative models.\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "FID = \\|\\mu_r - \\mu_g\\|^2 + \\operatorname{Tr}\\left(\\Sigma_r + \\Sigma_g - 2\\left(\\Sigma_r \\Sigma_g\\right)^{\\frac{1}{2}}\\right)\n",
    "$$  \n",
    "where:  \n",
    "- \\( \\mu_r, \\Sigma_r \\) are the mean and covariance of the real images' feature representations, and  \n",
    "- \\( \\mu_g, \\Sigma_g \\) are the mean and covariance of the generated images' feature representations.\n",
    "\n",
    "---\n",
    "\n",
    "### **Inception Score (IS)**  \n",
    "**What it tells:**  \n",
    "Evaluates both the quality and diversity of generated images by assessing the confidence of a pretrained classifier on generated images and the variety of classes represented.\n",
    "\n",
    "**Consideration:**  \n",
    "Higher scores indicate better performance but can sometimes be insensitive to mode collapse.\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "IS = \\exp\\left(\\mathbb{E}_{x}\\left[ D_{KL}(p(y|x) \\,\\|\\, p(y)) \\right]\\right)\n",
    "$$  \n",
    "where:  \n",
    "- \\( p(y|x) \\) is the conditional label distribution given image \\( x \\) from a pretrained classifier, and  \n",
    "- \\( p(y) \\) is the marginal distribution over all generated images.\n",
    "\n",
    "---\n",
    "\n",
    "### **Perplexity (for text generation)**  \n",
    "**What it tells:**  \n",
    "Measures how well a probability model predicts a sample. Lower perplexity indicates the model is more confident and makes fewer mistakes in predicting the next token in a sequence.\n",
    "\n",
    "**Consideration:**  \n",
    "Commonly used in language modeling; a lower perplexity suggests a better model fit.\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "\\text{Perplexity} = \\exp\\left(-\\frac{1}{N} \\sum_{i=1}^{N} \\log p(w_i)\\right)\n",
    "$$  \n",
    "or equivalently,\n",
    "$$\n",
    "\\text{Perplexity} = 2^{-\\frac{1}{N} \\sum_{i=1}^{N} \\log_2 p(w_i)}\n",
    "$$  \n",
    "where \\( p(w_i) \\) is the predicted probability of the \\( i \\)th token.\n",
    "\n",
    "---\n",
    "\n",
    "# Summary of Insights by Model Type\n",
    "\n",
    "- **Classification Metrics:**  \n",
    "  Focus on the correctness of discrete predictions using measures such as accuracy, precision, recall, F1 score, ROC-AUC, PR-AUC, and log loss.\n",
    "\n",
    "- **Regression Metrics:**  \n",
    "  Measure the magnitude and direction of errors using MAE, MSE, RMSE, \\(R^2\\), MAPE, and Huber Loss.\n",
    "\n",
    "- **Clustering Metrics:**  \n",
    "  Assess the cohesiveness and separation of data groups with metrics like Silhouette Score, Davies-Bouldin Index, Dunn Index, Adjusted Rand Index, and Normalized Mutual Information.\n",
    "\n",
    "- **Anomaly Detection Metrics:**  \n",
    "  Gauge the model’s ability to identify rare or unusual instances using adapted classification metrics (precision, recall, F1), ROC-AUC/PR-AUC, reconstruction error (MSE), and statistical distances (Z-score, Mahalanobis Distance).\n",
    "\n",
    "- **Ranking & Recommendation Metrics:**  \n",
    "  Evaluate the ordering and relevance of results using Mean Reciprocal Rank, NDCG, Hit Rate, and Mean Average Precision.\n",
    "\n",
    "- **Generative Model Metrics:**  \n",
    "  Measure the fidelity and diversity of generated content through metrics like FID, Inception Score, and Perplexity.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
