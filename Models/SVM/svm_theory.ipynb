{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video: [SVM's by StatQuest with visuals](https://www.youtube.com/watch?v=efR1C6CvhmE&list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF&index=72&ab_channel=StatQuestwithJoshStarmer)\n",
    "---\n",
    "\n",
    "# **Understanding Support Vector Machines (SVMs)**  \n",
    "\n",
    "## **1. Introduction: Classifying Mice by Mass**  \n",
    "We measured the mass of several mice:  \n",
    "- **Red dots** = Non-obese mice  \n",
    "- **Green dots** = Obese mice  \n",
    "\n",
    "We set a **threshold**:  \n",
    "- **Less than the threshold** â†’ **Not obese**  \n",
    "- **More than the threshold** â†’ **Obese**  \n",
    "\n",
    "### **Issue with a Simple Threshold**  \n",
    "What if a new observation is **closer** to the non-obese group but still falls above the threshold? This classification doesnâ€™t make sense.  \n",
    "\n",
    "### **Improving the Threshold**  \n",
    "Instead of using a random threshold, we:  \n",
    "1. **Identify edge observations** of each cluster.  \n",
    "2. **Use the midpoint** between these edge observations as the threshold.  \n",
    "\n",
    "Now, new observations are classified based on which group they are **closer to**.  \n",
    "\n",
    "---\n",
    "\n",
    "## **2. Understanding Margins**  \n",
    "### **What is a Margin?**  \n",
    "- The **shortest distance** between an observation and the threshold.  \n",
    "- If the threshold is exactly between two edge observations, the **margin is maximized**.  \n",
    "\n",
    "### **Why Maximize the Margin?**  \n",
    "- Moving the threshold **left** or **right** reduces the margin.  \n",
    "- The best threshold is the one with the **largest margin**.  \n",
    "- This is called a **Maximal Margin Classifier**.  \n",
    "\n",
    "**ðŸ”” Terminology Alert:**  \n",
    "A **Maximal Margin Classifier** is a classifier that **maximizes the margin** between the two groups.  \n",
    "\n",
    "---\n",
    "\n",
    "## **3. The Problem with Maximal Margin Classifiers**  \n",
    "What if thereâ€™s an **outlier** in the training data?  \n",
    "- The margin will shrink drastically.  \n",
    "- The classifier will be **too sensitive** to outliers.  \n",
    "- **New observations** may be misclassified.  \n",
    "\n",
    "### **Can We Do Better? Yes!**  \n",
    "We allow **some misclassifications** to create a **more robust threshold**.  \n",
    "\n",
    "---\n",
    "\n",
    "## **4. Soft Margins and the Bias-Variance Tradeoff**  \n",
    "### **Allowing Misclassifications**  \n",
    "Instead of forcing a strict margin, we allow:  \n",
    "- **Some misclassified points**.  \n",
    "- **Some correctly classified points** to be inside the margin.  \n",
    "\n",
    "This helps generalize the model and reduces overfitting.  \n",
    "\n",
    "### **Bias-Variance Tradeoff**  \n",
    "- **Strict classifiers** (no misclassifications) â†’ **Low bias, high variance**  \n",
    "- **Soft margin classifiers** (allow misclassifications) â†’ **Higher bias, lower variance**  \n",
    "\n",
    "ðŸ”” **Terminology Alert:**  \n",
    "- The **Soft Margin** is the new flexible margin that allows misclassifications.  \n",
    "- A **Soft Margin Classifier** is also called a **Support Vector Classifier (SVC)**.  \n",
    "\n",
    "---\n",
    "\n",
    "## **5. Support Vector Classifiers in Higher Dimensions**  \n",
    "### **What if We Have More Features?**  \n",
    "- **1D Data** â†’ Classifier is a **point**.  \n",
    "- **2D Data** â†’ Classifier is a **line**.  \n",
    "- **3D Data** â†’ Classifier is a **plane**.  \n",
    "- **4D+ Data** â†’ Classifier is a **hyperplane**.  \n",
    "\n",
    "ðŸ”” **Terminology Alert:**  \n",
    "A **Hyperplane** is a generalization of a classifier in **higher dimensions**.  \n",
    "\n",
    "---\n",
    "\n",
    "## **6. The Problem with Overlapping Data**  \n",
    "What if the **two groups overlap** heavily?  \n",
    "Example:  \n",
    "- **Drug dosages** and patient responses.  \n",
    "- The drug **only works in a certain range**.  \n",
    "\n",
    "A **Support Vector Classifier** struggles here. We need a **better solution**.  \n",
    "\n",
    "---\n",
    "\n",
    "## **7. Introduction to Support Vector Machines (SVMs)**  \n",
    "### **How Do We Improve?**  \n",
    "We introduce **Support Vector Machines (SVMs)**, which work in **higher dimensions**.  \n",
    "\n",
    "### **How Do SVMs Work?**  \n",
    "1. **Start with low-dimensional data.**  \n",
    "2. **Transform it into a higher-dimensional space.**  \n",
    "3. **Find a Support Vector Classifier in the higher dimension.**  \n",
    "\n",
    "---\n",
    "\n",
    "## **8. The Role of Kernel Functions**  \n",
    "### **Why Do We Need a Kernel Function?**  \n",
    "- We need a way to **transform data into higher dimensions**.  \n",
    "- Different transformations work for different problems.  \n",
    "\n",
    "### **Polynomial Kernel**  \n",
    "- Uses **polynomial functions** to add dimensions.  \n",
    "- The **degree (D)** controls the transformation.  \n",
    "  - **D = 1** â†’ No transformation.  \n",
    "  - **D = 2** â†’ Squared features.  \n",
    "  - **D = 3** â†’ Cubed features, etc.  \n",
    "- **Cross-validation** helps choose the best **D**.  \n",
    "\n",
    "### **Radial Basis Function (RBF) Kernel**  \n",
    "- Maps data into **infinite dimensions**.  \n",
    "- Works similarly to **weighted nearest neighbors**.  \n",
    "- **Close points** have more influence on classification.  \n",
    "\n",
    "ðŸ”” **Terminology Alert:**  \n",
    "The **Kernel Trick** allows us to compute high-dimensional relationships **without explicitly transforming the data**.  \n",
    "\n",
    "---\n",
    "\n",
    "## **9. Summary and Final Thoughts**  \n",
    "### **Key Takeaways:**  \n",
    "1. **Maximal Margin Classifier** maximizes the margin but is **too sensitive to outliers**.  \n",
    "2. **Soft Margin Classifiers (Support Vector Classifiers)** allow misclassifications for better generalization.  \n",
    "3. **Support Vector Machines (SVMs)** solve non-linear classification problems by mapping data into **higher dimensions** using **kernels**.  \n",
    "4. **Polynomial and Radial Kernels** systematically transform data to improve classification.  \n",
    "\n",
    "### **Final BAM!**  \n",
    "Support Vector Machines are **powerful tools** in machine learning when dealing with **complex, overlapping, and non-linearly separable data**.  \n",
    "\n",
    "**Quest on!** ðŸš€  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Understanding the Polynomial Kernel in Support Vector Machines (SVMs)**  \n",
    "\n",
    "The **polynomial kernel** is a method used in **Support Vector Machines (SVMs)** to classify data that is **not linearly separable** in its original form. Instead of trying to separate the data in its current **low-dimensional space**, we use a **polynomial function** to **map** the data into a **higher-dimensional space**, where it becomes easier to separate using a hyperplane.  \n",
    "\n",
    "---\n",
    "\n",
    "## **1. Polynomial Kernel Formula**  \n",
    "The polynomial kernel function is given by:  \n",
    "$\n",
    "K(A, B) = (A \\cdot B + R)^D\n",
    "$\n",
    "where:  \n",
    "- **A, B** â†’ Two feature vectors (observations) in the dataset.  \n",
    "- **A Â· B** â†’ The **dot product** of A and B (measuring similarity).  \n",
    "- **R** â†’ A constant (also called the bias term) that controls the influence of higher-order terms.  \n",
    "- **D** â†’ The degree of the polynomial (controls complexity and flexibility).  \n",
    "\n",
    "This function computes the relationship between every pair of points in the dataset as if they were mapped to a higher-dimensional space.  \n",
    "\n",
    "---\n",
    "\n",
    "## **2. Why Use the Polynomial Kernel?**  \n",
    "When data is **not linearly separable**, a simple linear classifier **(a straight line or a hyperplane in higher dimensions)** will not work. The polynomial kernel enables us to:  \n",
    "- **Map data into a higher-dimensional space** where separation is possible.  \n",
    "- **Avoid explicitly computing higher dimensions** using the **Kernel Trick** (discussed later).  \n",
    "\n",
    "For example, in a **one-dimensional dataset**, if we add a squared term (degree = 2), we **lift the data into a parabola shape** in **two dimensions**, making it easier to separate using a straight line.  \n",
    "\n",
    "---\n",
    "\n",
    "## **3. How the Polynomial Kernel Defines a Support Vector Classifier**  \n",
    "### **Step 1: Compute Kernel Values (Pairwise Relationships)**  \n",
    "- Instead of working directly in a higher dimension, we calculate the **kernel values** between all pairs of points.  \n",
    "- This means each data point is **implicitly transformed** into a higher-dimensional space **without actually computing the transformation explicitly**.  \n",
    "\n",
    "### **Step 2: Construct the Decision Boundary (Hyperplane)**  \n",
    "- Once the kernel values are calculated, we train an **SVM model** as usual.  \n",
    "- The SVM finds the **optimal hyperplane** in the high-dimensional space that **maximizes the margin** between classes.  \n",
    "\n",
    "### **Step 3: Classify New Observations**  \n",
    "- When a new observation arrives, its **kernel value** is computed with respect to the support vectors.  \n",
    "- The SVM then determines which **side of the hyperplane** the observation falls on and classifies it accordingly.  \n",
    "\n",
    "---\n",
    "\n",
    "## **4. Example: Using a Polynomial Kernel for Classification**\n",
    "Let's say we have **one-dimensional data** representing a drug dosage and whether it cured patients:  \n",
    "\n",
    "| Dosage | Cured (1) / Not Cured (0) |\n",
    "|--------|--------------------------|\n",
    "| 0.5    | 0                        |\n",
    "| 1.0    | 1                        |\n",
    "| 1.5    | 1                        |\n",
    "| 2.0    | 0                        |\n",
    "\n",
    "The data is **not linearly separable**, meaning we cannot draw a straight line to classify cured vs. not cured patients.  \n",
    "\n",
    "### **Transforming the Data Using a Polynomial Kernel**\n",
    "If we apply a **polynomial kernel with \\( D = 2 \\)** (squaring the dosage values), our new feature space becomes:  \n",
    "\n",
    "| Dosage (X) | Squared Dosage (XÂ²) | Transformed Space |\n",
    "|------------|----------------------|-------------------|\n",
    "| 0.5        | 0.25                 | (0.5, 0.25)      |\n",
    "| 1.0        | 1.00                 | (1.0, 1.00)      |\n",
    "| 1.5        | 2.25                 | (1.5, 2.25)      |\n",
    "| 2.0        | 4.00                 | (2.0, 4.00)      |\n",
    "\n",
    "Now, the data exists in a **higher-dimensional space**, where an SVM can find a **linear boundary (a straight line in 2D)** to separate cured vs. not cured patients.  \n",
    "\n",
    "---\n",
    "\n",
    "## **5. Kernel Trick: Computing High-Dimensional Relationships Efficiently**  \n",
    "The **kernel trick** is what makes polynomial kernels (and other kernel methods) computationally feasible.  \n",
    "\n",
    "### **What the Kernel Trick Does:**  \n",
    "- **Instead of explicitly transforming data into a higher dimension**, we compute the **pairwise kernel values** directly.  \n",
    "- This saves computational resources and allows SVMs to efficiently operate in **very high-dimensional spaces**.  \n",
    "\n",
    "For example, if we had **10,000 features**, explicitly computing all polynomial transformations would be computationally expensive. The kernel trick **avoids this problem** by computing the relationships **as if the transformation had been performed**.  \n",
    "\n",
    "---\n",
    "\n",
    "## **6. Choosing the Degree (D) and Coefficient (R)**\n",
    "### **Impact of D (Degree) on Model Complexity**\n",
    "- **Low degree (D = 1, 2):**  \n",
    "  - Less complex, fewer parameters.  \n",
    "  - Good for simple patterns.  \n",
    "- **Higher degree (D â‰¥ 3):**  \n",
    "  - More complex, flexible decision boundaries.  \n",
    "  - Can capture intricate relationships but may lead to **overfitting**.  \n",
    "\n",
    "### **Impact of R (Bias Term)**\n",
    "- Controls the impact of higher-order polynomial terms.  \n",
    "- Small **R** â†’ More influence from lower-degree terms.  \n",
    "- Large **R** â†’ Higher-degree terms have more impact, leading to more complex decision boundaries.  \n",
    "\n",
    "### **How to Choose D and R?**\n",
    "- We use **cross-validation** to test different values of **D** and **R** and select the combination that provides the best generalization on unseen data.  \n",
    "\n",
    "---\n",
    "\n",
    "## **7. Comparison with Other Kernels**\n",
    "| **Kernel**         | **Mathematical Form**              | **Use Case** |\n",
    "|--------------------|----------------------------------|-------------|\n",
    "| **Linear Kernel**  | $A \\cdot B  $               | Simple, linearly separable data |\n",
    "| **Polynomial Kernel**  | $(A \\cdot B + R)^D $ | Non-linear patterns, curved boundaries |\n",
    "| **Radial Basis Function (RBF) Kernel**  | $exp(âˆ’Î³âˆ¥Aâˆ’Bâˆ¥^2)$ | Highly complex, captures intricate relationships |\n",
    "| **Sigmoid Kernel**  | $\\tanh(A \\cdot B + R) $ | Similar to neural networks |\n",
    "\n",
    "---\n",
    "\n",
    "## **8. Summary**\n",
    "### **Key Takeaways:**\n",
    "- The **polynomial kernel** maps data into a **higher-dimensional space** to make classification easier.  \n",
    "- Instead of explicitly computing new feature spaces, we use the **kernel trick** to compute **pairwise relationships** efficiently.  \n",
    "- The **degree (D) and coefficient (R)** determine the complexity of the transformation.  \n",
    "- **Cross-validation** helps find the best values for D and R.  \n",
    "- The polynomial kernel is great for **moderate non-linearity**, but for highly complex data, **RBF kernels** may perform better.  \n",
    "\n",
    "### **Final BAM! ðŸš€**  \n",
    "The polynomial kernel is a powerful tool in **Support Vector Machines**, enabling them to classify **non-linearly separable data** efficiently!  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# **Understanding the Radial Basis Function (RBF) Kernel in Support Vector Machines (SVMs)**  \n",
    "\n",
    "## **Introduction**  \n",
    "We had a training dataset based on drug dosages measured in a group of patients.  \n",
    "- **Red dots** represented patients who were **not cured**.  \n",
    "- **Green dots** represented patients who were **cured**.  \n",
    "- The drug only worked when the dosage was **just right**â€”not too small or too large.  \n",
    "\n",
    "Because of the overlap in the data, we were **unable to find a satisfying support vector classifier (SVC)** to separate the cured from the non-cured patients.  \n",
    "\n",
    "## **Using the Radial Basis Function (RBF) Kernel**  \n",
    "One way to handle overlapping data is by using a **Support Vector Machine (SVM) with an RBF kernel**.  \n",
    "- The RBF kernel **finds support vector classifiers in infinite dimensions**, making it impossible to visualize.  \n",
    "- However, in practice, it **behaves like a weighted nearest neighbor model**:\n",
    "  - **Closest observations (nearest neighbors)** influence classification the most.  \n",
    "  - **Farther observations** have relatively little influence.  \n",
    "\n",
    "For example, if a new observation appears, the **nearest points** will dictate how it is classified.  \n",
    "\n",
    "## **How the RBF Kernel Determines Influence**  \n",
    "The RBF kernel measures how much influence one observation has on another.  \n",
    "\n",
    "### **Mathematical Representation of the RBF Kernel**  \n",
    "The RBF kernel is given by:  \n",
    "\n",
    "$\n",
    "K(A, B) = \\exp(-Î³ \\| A - B \\|^2)\n",
    "$\n",
    "\n",
    "where:  \n",
    "- $ A, B $ are different dosage measurements.  \n",
    "- $ \\| A - B \\|^2 $ is the **squared distance** between two points.  \n",
    "- \\( Î³ \\) (gamma) **scales the squared distance**, adjusting influence.  \n",
    "\n",
    "### **Impact of Gamma (\\( Î³ \\))**  \n",
    "- **When \\( Î³ = 1 \\):**  \n",
    "  - Plugging in two close observations gives **0.11**.  \n",
    "- **When \\( Î³ = 2 \\):**  \n",
    "  - Plugging in the same values results in **0.01** (less influence than when \\( Î³ = 1 \\)).  \n",
    "- **When two observations are far apart**, their influence approaches **0**.  \n",
    "\n",
    "Thus, **higher gamma values shrink influence to closer neighbors**, while **lower gamma values allow broader influence**.  \n",
    "\n",
    "## **How the RBF Kernel Works in Infinite Dimensions**  \n",
    "The key idea behind the RBF kernel is that it **projects data into infinite dimensions**, similar to **polynomial kernels**.  \n",
    "\n",
    "### **Polynomial Kernel Intuition**  \n",
    "A polynomial kernel can transform data into higher dimensions by adding polynomial terms.  \n",
    "- Example: If we use a **polynomial kernel with \\( R = 0 \\) and \\( D = 2 \\)**, the transformation becomes:  \n",
    "  $\n",
    "  \\text{New X-coordinate} = \\text{Dosage}^2\n",
    "  $\n",
    "  - This shifts data to a **new 2D space** where it is more separable.  \n",
    "- If we increase **\\( D \\) to 3**, the transformation becomes:  \n",
    "  $\n",
    "  \\text{New X-coordinate} = \\text{Dosage}^3\n",
    "  $\n",
    "  - Data shifts further in a **higher-dimensional space**.  \n",
    "\n",
    "Now, what if we **keep increasing \\( D \\) until infinity**?  \n",
    "- This creates an **infinite-dimensional transformation**, exactly what the RBF kernel does!  \n",
    "\n",
    "## **Deriving the RBF Kernel Using Taylor Series Expansion**  \n",
    "To formally prove that the **RBF kernel maps to infinite dimensions**, we use a **Taylor series expansion** of the exponential function.  \n",
    "\n",
    "### **Taylor Series Expansion of $e^x$**  \n",
    "$\n",
    "e^x = 1 + x + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\dots\n",
    "$\n",
    "- If we substitute \\( x = A \\cdot B \\), we get an **infinite sum of polynomial kernels**.  \n",
    "- The result is a **dot product in infinite dimensions**.  \n",
    "\n",
    "Thus, when we compute the RBF kernel, the value we get is the **high-dimensional relationship between two points** in an **infinite-dimensional space**!  \n",
    "\n",
    "## **Conclusion**  \n",
    "- The **RBF kernel transforms data into infinite dimensions**, allowing **complex decision boundaries**.  \n",
    "- It behaves like a **weighted nearest neighbor classifier**, where **gamma (\\( Î³ \\)) controls the influence of points**.  \n",
    "- **Mathematically, it is derived from polynomial expansions using the Taylor series**.  \n",
    "- **Final takeaway:** The RBF kernel is **powerful for handling non-linear data** that is not linearly separable in lower dimensions.  \n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
